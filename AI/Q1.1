Question 1.1 : Load the Dataset

Explanation: Load the Iris dataset from scikit-learn’s datasets module and Display the first 5 rows of the dataset to understand its structure.



from sklearn.datasets import load_iris
import pandas as pd
iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target
iris_df.head()



Question 2.1 : Exploratory Data Analysis (EDA)

Explanation:
• Print the summary statistics of the dataset (mean, median, mode, standard deviation, etc.).
• Check for any missing values in the dataset and handle them appropriately.
• Plot the distribution of each feature using histograms.
• Visualize the pairwise relationships between features using a pair plot (or scatter plot matrix).




import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris
iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target

print("Summary Statistics:")
print(iris_df.describe())   
print("\nMedian:")
print(iris_df.median())    
print("\nMode:")
print(iris_df.mode())

print("\nChecking for missing values:")
print(iris_df.isnull().sum())

iris_df.hist(bins=20, figsize=(12, 8))
plt.suptitle("Distribution of Features")
plt.show()

sns.pairplot(iris_df, hue='species', diag_kind='hist')
plt.suptitle("Pairwise Relationships Between Features", y=1.02)
plt.show()



Question 3.1 : Feature Scaling

Explanation:
• Standardize the features by removing the mean and scaling to unit variance using StandardScaler from scikit-learn.
• Alternatively, perform Min-Max scaling on the features using MinMaxScaler.



from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd

from sklearn.datasets import load_iris
iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

scaler_standard = StandardScaler()
iris_standardized = scaler_standard.fit_transform(iris_df)

iris_standardized_df = pd.DataFrame(iris_standardized, columns=iris.feature_names)
print("Standardized Features:")
print(iris_standardized_df.head()) 

scaler_minmax = MinMaxScaler()
iris_minmax_scaled = scaler_minmax.fit_transform(iris_df)

iris_minmax_scaled_df = pd.DataFrame(iris_minmax_scaled, columns=iris.feature_names)
print("\nMin-Max Scaled Features:")
print(iris_minmax_scaled_df.head())




Question 3.2 : Encoding the Target Variable

Explanation:
• Encode the categorical target variable (species) into numeric values using label encoding.

import pandas as pd
from sklearn.preprocessing import LabelEncoder

from sklearn.datasets import load_iris
iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

iris_df['species'] = iris.target

label_encoder = LabelEncoder()
iris_df['species_encoded'] = label_encoder.fit_transform(iris_df['species'])

print(iris_df[['species', 'species_encoded']].head())

print("\nMapping of species to numeric values:")
species_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print(species_mapping)




Question 4.1 : Splitting the Dataset

Explanation:
• Split the dataset into training and testing sets using an 80-20 split. Use train_test_split from scikit-learn.

from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target

X = iris_df.drop(columns=['species'])
y = iris_df['species']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training feature set shape: {X_train.shape}")
print(f"Testing feature set shape: {X_test.shape}")
print(f"Training target set shape: {y_train.shape}")
print(f"Testing target set shape: {y_test.shape}")






Question 5.1 : Principal Component Analysis (PCA)

Explanation:
• Perform PCA on the Iris dataset to reduce the dimensionality to 2 components.
• Plot the data points in the new 2D space with different colors for each species.



from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target

X = iris_df.drop(columns=['species'])
y = iris_df['species']

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['species'] = y

plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue=pca_df['species'], palette='Set1', data=pca_df, legend='full')
plt.title('PCA of Iris Dataset (2 Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Species', loc='best', labels=iris.target_names)
plt.show()
